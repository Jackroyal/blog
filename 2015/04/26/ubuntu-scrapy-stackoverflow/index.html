<!DOCTYPE HTML>
<html>
<head>
  <meta charset="utf-8">
  
  <title>ubuntu下使用scrapy抓取cnblogs | 搁浅St的blog</title>
  <meta name="author" content="搁浅St">
  
  <meta name="description" content="jackroyal 博客 搁浅St 笨笨">
  
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  <meta property="og:title" content="ubuntu下使用scrapy抓取cnblogs"/>
  <meta property="og:site_name" content="搁浅St的blog"/>

  
    <meta property="og:image" content="undefined"/>
  

  <link href="/favicon.png" rel="icon">
  <link rel="alternate" href="/atom.xml" title="搁浅St的blog" type="application/atom+xml">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  <!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->
  

</head>


<body>
  <header id="header" class="inner"><div class="alignleft">
  <h1><a href="/">搁浅St的blog</a></h1>
  <h2><a href="/">我最喜欢笨笨</a></h2>
</div>
<nav id="main-nav" class="alignright">
  <ul>
    
      <li><a href="/">Home</a></li>
    
      <li><a href="/archives">Archives</a></li>
    
    <li> <a href="/atom.xml">RSS</a> </li>
  </ul>
  <div class="clearfix"></div>
</nav>
<div class="clearfix"></div>
</header>
  <div id="content" class="inner">
    <div id="main-col" class="alignleft"><div id="wrapper"><article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2015-04-26T08:52:28.000Z"><a href="/2015/04/26/ubuntu-scrapy-stackoverflow/">4月 26 2015</a></time>
      
      
  
    <h1 class="title">ubuntu下使用scrapy抓取cnblogs</h1>
  

    </header>
    <div class="entry">
      
        <p>今天在伯乐在线上看到一篇翻译的博客，讲的是使用scrapy来抓取stackoverflow上的问题，刚好好久没用这个，于是一并捡起来玩一下。<br><a id="more"></a></p>
<h1 id="软件安装">软件安装</h1>
<p>我的环境是：ubuntu 14.04 lts<br>需要安装相关软件</p>
<h2 id="scrapy">scrapy</h2>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">pip <span class="operator"><span class="keyword">install</span> Scrapy</span></div></pre></td></tr></table></figure>

<h2 id="PyMongo">PyMongo</h2>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">pip <span class="operator"><span class="keyword">install</span> pymongo</span></div></pre></td></tr></table></figure>

<h2 id="Mongodb">Mongodb</h2>
<p>上面安装的是python使用Mongodb的接口，很显然，我们要安装Mongodb才能使用</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sudo apt-get <span class="operator"><span class="keyword">install</span> mongodb-<span class="keyword">server</span></span></div></pre></td></tr></table></figure>

<p>至此，要使用的软件都已经安装完毕</p>
<h1 id="使用scrapy新建工程">使用scrapy新建工程</h1>
<p>使用scrapy新建工程很简单，如下所示，我们新建一个stack的项目，他会在你的当前目录新建一个stack文件夹</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="title">scrapy</span> startproject stack</div></pre></td></tr></table></figure>

<p>并且会建成如下所示的目录树结构</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">chen<span class="variable">@chen</span>-<span class="constant">P31</span><span class="symbol">:~</span><span class="variable">$ </span>tree stack</div><div class="line">stack</div><div class="line">├── stack</div><div class="line">│   ├── __init_<span class="number">_</span>.py</div><div class="line">│   ├── items.py</div><div class="line">│   ├── pipelines.py</div><div class="line">│   ├── settings.py</div><div class="line">│   └── spiders</div><div class="line">│       └── __init_<span class="number">_</span>.py</div><div class="line">└── scrapy.cfg</div></pre></td></tr></table></figure>

<p>接下来，我们修改items.py的内容，这个文件用于定义存储“容器”，用来存储将要抓取的数据。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> scrapy.item <span class="keyword">import</span> Item,Field</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">StackItem</span><span class="params">(Item)</span>:</span></div><div class="line">    <span class="comment"># define the fields for your item here like:</span></div><div class="line">    <span class="comment"># name = scrapy.Field()</span></div><div class="line">    title = Field()<span class="comment">#我们添加两个字段，我们等会儿会抓取一个标题和url两个字段</span></div><div class="line">    url = Field()</div></pre></td></tr></table></figure>

<p>接着，还有一个很重要的东西，对，就是我们的蜘蛛，我们在spider目录下，新建一个stack_spider.py文件。顾名思义，这就是我们的蜘蛛。我们需要定义我们爬虫的起点，爬虫的规则等等</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> scrapy <span class="keyword">import</span> Spider</div><div class="line"><span class="keyword">from</span> stack.items <span class="keyword">import</span> StackItem  <span class="comment">#导入我们上面定义的容器类</span></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">StackSpider</span><span class="params">(Spider)</span>:</span></div><div class="line">    name = <span class="string">'stack'</span>   <span class="comment">#定义我们爬虫的名字</span></div><div class="line">    allowed_domains = [<span class="string">"cnblogs.com"</span>]   <span class="comment">#规定爬虫爬取的域名</span></div><div class="line">    start_urls = [<span class="string">'http://www.cnblogs.com/geqianst/p/'</span>,]   <span class="comment">#爬虫工作的起点</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span><span class="comment">#爬虫用来做数据解析的</span></div><div class="line">        questions = response.xpath(<span class="string">'//div[@id="myposts"]//a[@id]'</span>)</div><div class="line">        <span class="comment">#xpath选择器，这里的含义是取所有id为myposts的div，在它下面找所有带id的超链接a</span></div><div class="line">        <span class="comment">#实际结果是这样的</span></div><div class="line">        <span class="comment">#[&lt;Selector xpath='//div[@id="myposts"]//a[@id]' data=u'&lt;a id="PostsList1_rpPosts_TitleUrl_0" hr'&gt;,</span></div><div class="line">        <span class="comment">#&lt;Selector xpath='//div[@id="myposts"]//a[@id]' data=u'&lt;a id="PostsList1_rpPosts_TitleUrl_1" hr'&gt;,</span></div><div class="line">        <span class="comment">#&lt;Selector xpath='//div[@id="myposts"]//a[@id]' data=u'&lt;a id="PostsList1_rpPosts_TitleUrl_2" hr'&gt;,</span></div><div class="line">        <span class="comment">#&lt;Selector xpath='//div[@id="myposts"]//a[@id]' data=u'&lt;a id="PostsList1_rpPosts_TitleUrl_3" hr'&gt;,</span></div><div class="line">        <span class="comment">#&lt;Selector xpath='//div[@id="myposts"]//a[@id]' data=u'&lt;a id="PostsList1_rpPosts_TitleUrl_4" hr'&gt;,</span></div><div class="line">        <span class="comment">#&lt;Selector xpath='//div[@id="myposts"]//a[@id]' data=u'&lt;a id="PostsList1_rpPosts_TitleUrl_5" hr'&gt;,</span></div><div class="line">        <span class="comment">#&lt;Selector xpath='//div[@id="myposts"]//a[@id]' data=u'&lt;a id="PostsList1_rpPosts_TitleUrl_6" hr'&gt;,</span></div><div class="line">        <span class="comment">#&lt;Selector xpath='//div[@id="myposts"]//a[@id]' data=u'&lt;a id="PostsList1_rpPosts_TitleUrl_7" hr'&gt;,</span></div><div class="line">        <span class="comment">#&lt;Selector xpath='//div[@id="myposts"]//a[@id]' data=u'&lt;a id="PostsList1_rpPosts_TitleUrl_8" hr'&gt;,</span></div><div class="line">        <span class="comment">#&lt;Selector xpath='//div[@id="myposts"]//a[@id]' data=u'&lt;a id="PostsList1_rpPosts_TitleUrl_9" hr'&gt;]</span></div><div class="line">        <span class="comment">#</span></div><div class="line"></div><div class="line">        <span class="keyword">for</span> question <span class="keyword">in</span> questions:</div><div class="line">            item = StackItem()</div><div class="line">            item[<span class="string">'title'</span>] = question.xpath(</div><div class="line">                <span class="string">'text()'</span>).extract()[<span class="number">0</span>]</div><div class="line">            item[<span class="string">'url'</span>] = question.xpath(</div><div class="line">                <span class="string">'@href'</span>).extract()[<span class="number">0</span>]</div><div class="line">            <span class="keyword">print</span> item</div><div class="line">            <span class="keyword">yield</span> item</div></pre></td></tr></table></figure>

<h1 id="测试">测试</h1>
<p>ok，上述工作基本完成，我们来测试一下</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">scrapy crawl stack</div></pre></td></tr></table></figure>

<p>还可以这样测试一下，使用shell命令<br><img src="http://ww4.sinaimg.cn/large/692869a3gw1erjvd7qeqdj213z0j9h2h.jpg" alt="用shell测试xpath"><br>妈蛋，我的竟然出错了，输出如下</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line">chen<span class="variable">@chen</span>-<span class="constant">P31</span><span class="symbol">:~/stack</span><span class="variable">$ </span>scrapy crawl stack</div><div class="line"><span class="number">2015</span>-<span class="number">04</span>-<span class="number">26</span> <span class="number">16</span><span class="symbol">:</span><span class="number">49</span><span class="symbol">:</span><span class="number">11</span>+080<span class="number">0</span> [scrapy] <span class="constant">INFO</span><span class="symbol">:</span> <span class="constant">Scrapy</span> <span class="number">0</span>.<span class="number">24.6</span> started (<span class="symbol">bot:</span> stack)</div><div class="line"><span class="number">2015</span>-<span class="number">04</span>-<span class="number">26</span> <span class="number">16</span><span class="symbol">:</span><span class="number">49</span><span class="symbol">:</span><span class="number">11</span>+080<span class="number">0</span> [scrapy] <span class="constant">INFO</span><span class="symbol">:</span> <span class="constant">Optional</span> features <span class="symbol">available:</span> ssl, http11</div><div class="line"><span class="number">2015</span>-<span class="number">04</span>-<span class="number">26</span> <span class="number">16</span><span class="symbol">:</span><span class="number">49</span><span class="symbol">:</span><span class="number">11</span>+080<span class="number">0</span> [scrapy] <span class="constant">INFO</span><span class="symbol">:</span> <span class="constant">Overridden</span> <span class="symbol">settings:</span> {<span class="string">'NEWSPIDER_MODULE'</span><span class="symbol">:</span> <span class="string">'stack.spiders'</span>, <span class="string">'SPIDER_MODULES'</span><span class="symbol">:</span> [<span class="string">'stack.spiders'</span>], <span class="string">'BOT_NAME'</span><span class="symbol">:</span> <span class="string">'stack'</span>}</div><div class="line"><span class="constant">Traceback</span> (most recent call last)<span class="symbol">:</span></div><div class="line">  <span class="constant">File</span> <span class="string">"/usr/local/bin/scrapy"</span>, line <span class="number">11</span>, <span class="keyword">in</span> &lt;<span class="class"><span class="keyword">module</span>&gt;</span></div><div class="line">    sys.exit(execute())</div><div class="line">  <span class="constant">File</span> <span class="string">"/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py"</span>, line <span class="number">143</span>, <span class="keyword">in</span> execute</div><div class="line">    _run_print_help(parser, _run_command, cmd, args, opts)</div><div class="line">  <span class="constant">File</span> <span class="string">"/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py"</span>, line <span class="number">89</span>, <span class="keyword">in</span> _run_print_help</div><div class="line">    func(*a, **kw)</div><div class="line">  <span class="constant">File</span> <span class="string">"/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py"</span>, line <span class="number">150</span>, <span class="keyword">in</span> _run_command</div><div class="line">    cmd.run(args, opts)</div><div class="line">  <span class="constant">File</span> <span class="string">"/usr/local/lib/python2.7/dist-packages/scrapy/commands/crawl.py"</span>, line <span class="number">60</span>, <span class="keyword">in</span> run</div><div class="line">    <span class="keyword">self</span>.crawler_process.start()</div><div class="line">  <span class="constant">File</span> <span class="string">"/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py"</span>, line <span class="number">92</span>, <span class="keyword">in</span> start</div><div class="line">    <span class="keyword">if</span> <span class="keyword">self</span>.start_crawling()<span class="symbol">:</span></div><div class="line">  <span class="constant">File</span> <span class="string">"/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py"</span>, line <span class="number">124</span>, <span class="keyword">in</span> start_crawling</div><div class="line">    <span class="keyword">return</span> <span class="keyword">self</span>._start_crawler() is <span class="keyword">not</span> <span class="constant">None</span></div><div class="line">  <span class="constant">File</span> <span class="string">"/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py"</span>, line <span class="number">139</span>, <span class="keyword">in</span> _start_crawler</div><div class="line">    crawler.configure()</div><div class="line">  <span class="constant">File</span> <span class="string">"/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py"</span>, line <span class="number">46</span>, <span class="keyword">in</span> configure</div><div class="line">    <span class="keyword">self</span>.extensions = <span class="constant">ExtensionManager</span>.from_crawler(<span class="keyword">self</span>)</div><div class="line">  <span class="constant">File</span> <span class="string">"/usr/local/lib/python2.7/dist-packages/scrapy/middleware.py"</span>, line <span class="number">50</span>, <span class="keyword">in</span> from_crawler</div><div class="line">    <span class="keyword">return</span> cls.from_settings(crawler.settings, crawler)</div><div class="line">  <span class="constant">File</span> <span class="string">"/usr/local/lib/python2.7/dist-packages/scrapy/middleware.py"</span>, line <span class="number">29</span>, <span class="keyword">in</span> from_settings</div><div class="line">    mwcls = load_object(clspath)</div><div class="line">  <span class="constant">File</span> <span class="string">"/usr/local/lib/python2.7/dist-packages/scrapy/utils/misc.py"</span>, line <span class="number">42</span>, <span class="keyword">in</span> load_object</div><div class="line">    raise <span class="constant">ImportError</span>(<span class="string">"Error loading object '%s': %s"</span> % (path, e))</div><div class="line"><span class="constant">ImportError</span><span class="symbol">:</span> <span class="constant">Error</span> loading object <span class="string">'scrapy.telnet.TelnetConsole'</span><span class="symbol">:</span> <span class="constant">No</span> <span class="class"><span class="keyword">module</span> <span class="title">named</span> <span class="title">conch</span></span></div></pre></td></tr></table></figure>

<p>这是什么gui？<br>还好我有stackoverflow，google一番，找到解决办法（其实这不是最后的解决办法，请往后看）<br>网上说是twisted的问题，重新安装一下就好了，ok，走起</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div></pre></td><td class="code"><pre><div class="line">chen@chen-P31:~/stack$ sudo apt-get install twisted</div><div class="line">Reading package lists... Done</div><div class="line">Building dependency tree       </div><div class="line">Reading state information... Done</div><div class="line">E: Unable to locate package twisted</div><div class="line">chen@chen-P31:~/stack$ sudo apt-get install twisted.conch</div><div class="line">Reading package lists... Done</div><div class="line">Building dependency tree       </div><div class="line">Reading state information... Done</div><div class="line">Note, selecting <span class="string">'python-twisted-conch'</span> <span class="keyword">for</span> regex <span class="string">'twisted.conch'</span></div><div class="line">Note, selecting <span class="string">'python2.7-twisted-conch'</span> <span class="keyword">for</span> regex <span class="string">'twisted.conch'</span></div><div class="line">Note, selecting <span class="string">'python-twisted-conch'</span> instead of <span class="string">'python2.7-twisted-conch'</span></div><div class="line">The following packages were automatically installed and are no longer required:</div><div class="line">  cli-common dockmanager freepats gstreamer1.0-plugins-bad-faad</div><div class="line">  gstreamer1.0-plugins-bad-videoparsers libbotan-<span class="number">1.10</span>-<span class="number">0</span>:i386 libcdaudio1</div><div class="line">  libdbus-glib2.0-cil libdbus2.0-cil libdbusmenu-glib4:i386</div><div class="line">  libdbusmenu-gtk4:i386 libegl1-mesa:i386 libegl1-mesa-drivers:i386 libflite1</div><div class="line">  libfluidsynth1 libgbm1:i386 libgconf2.0-cil libgdiplus libgif4</div><div class="line">  libgles2-mesa:i386 libglib2.0-cil libgme0 libgmp10:i386</div><div class="line">  libgnome-desktop-<span class="number">2</span>-<span class="number">17</span> libgnome-keyring1.0-cil libgnomedesktop2.20-cil</div><div class="line">  libgstreamer-plugins-bad0.10-<span class="number">0</span> libgstreamer-plugins-bad1.0-<span class="number">0</span> libgtk2.0-cil</div><div class="line">  libicu52:i386 libmimic0 libmms0 libmono-addins0.2-cil libmono-cairo4.0-cil</div><div class="line">  libmono-corlib4.0-cil libmono-corlib4.5-cil libmono-data-tds4.0-cil</div><div class="line">  libmono-i18n-west4.0-cil libmono-i18n4.0-cil libmono-posix4.0-cil</div><div class="line">  libmono-security4.0-cil libmono-sharpzip4.84-cil libmono-sqlite4.0-cil</div><div class="line">  libmono-system-configuration4.0-cil libmono-system-core4.0-cil</div><div class="line">  libmono-system-data4.0-cil libmono-system-drawing4.0-cil</div><div class="line">  libmono-system-enterpriseservices4.0-cil</div><div class="line">  libmono-system-runtime-serialization-formatters-soap4.0-cil</div><div class="line">  libmono-system-security4.0-cil libmono-system-transactions4.0-cil</div><div class="line">  libmono-system-web-applicationservices4.0-cil</div><div class="line">  libmono-system-web-services4.0-cil libmono-system-web4.0-cil</div><div class="line">  libmono-system-xml-linq4.0-cil libmono-system-xml4.0-cil</div><div class="line">  libmono-system4.0-cil libmono-web4.0-cil libmpg123-<span class="number">0</span> libnotify0.4-cil</div><div class="line">  libofa0 libopenal-data libopenal1 libopenvg1-mesa:i386 libqrencode3:i386</div><div class="line">  libqt5core5a:i386 libqt5dbus5:i386 libqt5gui5:i386 libqt5network5:i386</div><div class="line">  libqt5widgets5:i386 libqtshadowsocks:i386 librsvg2-<span class="number">2.18</span>-cil libslv2-<span class="number">9</span></div><div class="line">  libsoundtouch0 libspandsp2 libsrtp0 libssl1.0.0:i386 libv4l-<span class="number">0</span>:i386</div><div class="line">  libv4lconvert0:i386 libvo-aacenc0 libvo-amrwbenc0 libwayland-client0:i386</div><div class="line">  libwayland-egl1-mesa:i386 libwayland-server0:i386 libwildmidi-config</div><div class="line">  libwildmidi1 libwnck2.20-cil libxcb-icccm4:i386 libxcb-image0:i386</div><div class="line">  libxcb-keysyms1:i386 libxcb-randr0:i386 libxcb-render-util0:i386</div><div class="line">  libxcb-shape0:i386 libxcb-util0:i386 libxcb-xfixes0:i386 libxcb-xkb1:i386</div><div class="line">  libxkbcommon-x11-<span class="number">0</span>:i386 libxkbcommon0:i386 libzbar0:i386 mono-<span class="number">4.0</span>-gac</div><div class="line">  mono-gac mono-runtime mono-runtime-common mono-runtime-sgen python-mpd</div><div class="line">  python-mutagen python-twisted-names</div><div class="line">Use <span class="string">'apt-get autoremove'</span> to remove them.</div><div class="line">The following extra packages will be installed:</div><div class="line">  python-pyasn1</div><div class="line">The following NEW packages will be installed:</div><div class="line">  python-pyasn1 python-twisted-conch</div><div class="line"><span class="number">0</span> upgraded, <span class="number">2</span> newly installed, <span class="number">0</span> to remove and <span class="number">6</span> not upgraded.</div><div class="line">Need to get <span class="number">286</span> kB of archives.</div><div class="line">After this operation, <span class="number">1</span>,<span class="number">793</span> kB of additional disk space will be used.</div><div class="line">Do you want to continue? [Y/n] </div><div class="line">Get:<span class="number">1</span> http://mirrors.ustc.edu.cn/ubuntu/ trusty/main python-pyasn1 all <span class="number">0.1</span><span class="number">.7</span>-1ubuntu2 [<span class="number">44.2</span> kB]</div><div class="line">Get:<span class="number">2</span> http://mirrors.ustc.edu.cn/ubuntu/ trusty/main python-twisted-conch all <span class="number">1</span>:<span class="number">13.2</span><span class="number">.0</span>-1ubuntu1 [<span class="number">242</span> kB]</div><div class="line">Fetched <span class="number">286</span> kB <span class="keyword">in</span> 0s (<span class="number">1</span>,<span class="number">595</span> kB/s)         </div><div class="line">Selecting previously unselected package python-pyasn1.</div><div class="line">(Reading database <span class="keyword">...</span> <span class="number">359746</span> files and directories currently installed.)</div><div class="line">Preparing to unpack <span class="keyword">...</span>/python-pyasn1_0.1.7-1ubuntu2_all.deb <span class="keyword">...</span></div><div class="line">Unpacking python-pyasn1 (<span class="number">0.1</span><span class="number">.7</span>-1ubuntu2) <span class="keyword">...</span></div><div class="line">Selecting previously unselected package python-twisted-conch.</div><div class="line">Preparing to unpack <span class="keyword">...</span>/python-twisted-conch_1%3a13.2.0-1ubuntu1_all.deb <span class="keyword">...</span></div><div class="line">Unpacking python-twisted-conch (<span class="number">1</span>:<span class="number">13.2</span><span class="number">.0</span>-1ubuntu1) <span class="keyword">...</span></div><div class="line">Processing triggers <span class="keyword">for</span> doc-base (<span class="number">0.10</span><span class="number">.5</span>) <span class="keyword">...</span></div><div class="line">Processing <span class="number">1</span> added doc-base file...</div><div class="line">Processing triggers <span class="keyword">for</span> man-db (<span class="number">2.6</span><span class="number">.7</span><span class="number">.1</span>-1ubuntu1) <span class="keyword">...</span></div><div class="line">Setting up python-pyasn1 (<span class="number">0.1</span><span class="number">.7</span>-1ubuntu2) <span class="keyword">...</span></div><div class="line">Setting up python-twisted-conch (<span class="number">1</span>:<span class="number">13.2</span><span class="number">.0</span>-1ubuntu1) <span class="keyword">...</span></div></pre></td></tr></table></figure>

<p>安装总算完成，再试一次，妈蛋，又来一个新错误，这是什么gui？？？</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line">chen@chen-P31:~/stack$ scrapy crawl stack</div><div class="line"><span class="number">2015</span>-<span class="number">04</span>-<span class="number">26</span> <span class="number">16</span>:<span class="number">50</span>:<span class="number">44</span>+<span class="number">0800</span> [scrapy] INFO: Scrapy <span class="number">0.24</span>.<span class="number">6</span> started (bot: stack)</div><div class="line"><span class="number">2015</span>-<span class="number">04</span>-<span class="number">26</span> <span class="number">16</span>:<span class="number">50</span>:<span class="number">44</span>+<span class="number">0800</span> [scrapy] INFO: Optional features available: ssl, http11</div><div class="line"><span class="number">2015</span>-<span class="number">04</span>-<span class="number">26</span> <span class="number">16</span>:<span class="number">50</span>:<span class="number">44</span>+<span class="number">0800</span> [scrapy] INFO: Overridden settings: {<span class="string">'NEWSPIDER_MODULE'</span>: <span class="string">'stack.spiders'</span>, <span class="string">'SPIDER_MODULES'</span>: [<span class="string">'stack.spiders'</span>], <span class="string">'BOT_NAME'</span>: <span class="string">'stack'</span>}</div><div class="line">Traceback (most recent call last):</div><div class="line">  File <span class="string">"/usr/local/bin/scrapy"</span>, line <span class="number">11</span>, <span class="keyword">in</span> &lt;module&gt;</div><div class="line">    sys.exit(execute())</div><div class="line">  File <span class="string">"/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py"</span>, line <span class="number">143</span>, <span class="keyword">in</span> execute</div><div class="line">    _run_print_<span class="built_in">help</span>(parser, _run_command, cmd, args, opts)</div><div class="line">  File <span class="string">"/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py"</span>, line <span class="number">89</span>, <span class="keyword">in</span> _run_print_<span class="built_in">help</span></div><div class="line">    func(*a, **kw)</div><div class="line">  File <span class="string">"/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py"</span>, line <span class="number">150</span>, <span class="keyword">in</span> _run_command</div><div class="line">    cmd.run(args, opts)</div><div class="line">  File <span class="string">"/usr/local/lib/python2.7/dist-packages/scrapy/commands/crawl.py"</span>, line <span class="number">60</span>, <span class="keyword">in</span> run</div><div class="line">    self.crawler_process.start()</div><div class="line">  File <span class="string">"/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py"</span>, line <span class="number">92</span>, <span class="keyword">in</span> start</div><div class="line">    <span class="keyword">if</span> self.start_crawling():</div><div class="line">  File <span class="string">"/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py"</span>, line <span class="number">124</span>, <span class="keyword">in</span> start_crawling</div><div class="line">    <span class="keyword">return</span> self._start_crawler() is not None</div><div class="line">  File <span class="string">"/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py"</span>, line <span class="number">139</span>, <span class="keyword">in</span> _start_crawler</div><div class="line">    crawler.configure()</div><div class="line">  File <span class="string">"/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py"</span>, line <span class="number">46</span>, <span class="keyword">in</span> configure</div><div class="line">    self.extensions = ExtensionManager.from_crawler(self)</div><div class="line">  File <span class="string">"/usr/local/lib/python2.7/dist-packages/scrapy/middleware.py"</span>, line <span class="number">50</span>, <span class="keyword">in</span> from_crawler</div><div class="line">    <span class="keyword">return</span> cls.from_settings(crawler.settings, crawler)</div><div class="line">  File <span class="string">"/usr/local/lib/python2.7/dist-packages/scrapy/middleware.py"</span>, line <span class="number">29</span>, <span class="keyword">in</span> from_settings</div><div class="line">    mwcls = load_object(clspath)</div><div class="line">  File <span class="string">"/usr/local/lib/python2.7/dist-packages/scrapy/utils/misc.py"</span>, line <span class="number">42</span>, <span class="keyword">in</span> load_object</div><div class="line">    raise ImportError(<span class="string">"Error loading object '%s': %s"</span> % (path, e))</div><div class="line">ImportError: Error loading object <span class="string">'scrapy.contrib.memusage.MemoryUsage'</span>: No module named mail.smtp</div></pre></td></tr></table></figure>

<p>最后的最后，我在我们万能的github上找到<a href="https://github.com/scrapy/scrapy/issues/958" target="_blank" rel="external">答案</a>，原来是我们没有安装python-twisted，安装一下，世界都美好了</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sudo apt-<span class="built_in">get</span> install <span class="keyword">python</span>-twisted</div></pre></td></tr></table></figure>

<h2 id="输出到文件">输出到文件</h2>
<p>为了更直观的看到结果，我们将结果输出到一个json文件</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">scrapy crawl stack -<span class="keyword">o</span> <span class="built_in">items</span>.json -<span class="keyword">t</span> json</div></pre></td></tr></table></figure>

<p>噢耶，第一个爬虫成功</p>
<h1 id="存储到mongodb">存储到mongodb</h1>
<p>接下来，我们做最后一件事，我们将结果存储到mongodb的数据库中<br>在这里，我遇到一个大坑，无论是伯乐在线翻译的博客<br>还是网上搜索到的一般教程，都是使用pymongo.Connection来连接数据库，可是妈蛋，你使用<code>pip install pymongo</code>安装的版本都是最新版本3.0.1，那个Connection的写法已经不支持，被丢弃了，擦。<br>我们来看一下版本，我学到一个新命令<code>pip show pymongo</code>，用来查看某一个包的版本的。<br><img src="http://ww4.sinaimg.cn/large/692869a3gw1erjct36jnrj20df038dgf.jpg" alt="查看pymongo版本"><br>在pymongo 3.0的版本中，已经不再支持pymongo.Connection，而是使用pymongo.MongoClient来替代。</p>
<h2 id="第一步">第一步</h2>
<p>创建一个用来保存我们抓取数据的数据库。打开<code>settings.py</code>,指定管道，然后加入数据库的相关设置</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">BOT_NAME = <span class="string">'stack'</span></div><div class="line"></div><div class="line">SPIDER_MODULES = [<span class="string">'stack.spiders'</span>]</div><div class="line">NEWSPIDER_MODULE = <span class="string">'stack.spiders'</span></div><div class="line"></div><div class="line"><span class="comment"># Crawl responsibly by identifying yourself (and your website) on the user-agent</span></div><div class="line"><span class="comment">#USER_AGENT = 'stack (+http://www.yourdomain.com)'</span></div><div class="line"></div><div class="line">ITEM_PIPELINES = [<span class="string">'stack.pipelines.MongoDBPipeline'</span>, ]</div><div class="line"><span class="comment">#关于mongodb的相关设置，包括服务器的ip，端口号，数据库名，表名，</span></div><div class="line"><span class="comment">#我也是第一次使用mongodb竟然不需要用户验证信息，而且这表名确实奇怪，叫做MONGODB_COLLECTION</span></div><div class="line">MONGODB_SERVER = <span class="string">"localhost"</span></div><div class="line">MONGODB_PORT = <span class="number">27017</span></div><div class="line">MONGODB_DB = <span class="string">"stackoverflow"</span></div><div class="line">MONGODB_COLLECTION = <span class="string">"questions"</span></div><div class="line"></div><div class="line">DOWNLOAD_DELAY = <span class="number">5</span>  <span class="comment">#抓取的延迟</span></div></pre></td></tr></table></figure>

<h2 id="第二步">第二步</h2>
<p>我们已经能够爬取和解析html数据了，而且已经配置了数据库，接下来，我们通过<code>pipelines.py</code>中建立一个管道去连接这两个部分。<br>我们首先来完成数据库的连接部分</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pymongo</div><div class="line"></div><div class="line"><span class="keyword">from</span> scrapy.conf <span class="keyword">import</span> settings</div><div class="line"><span class="keyword">from</span> scrapy.exceptions <span class="keyword">import</span> DropItem</div><div class="line"><span class="keyword">from</span> scrapy <span class="keyword">import</span> log</div><div class="line"></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">MongoDBPipeline</span><span class="params">(object)</span>:</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></div><div class="line">        connection = pymongo.MongoClient(</div><div class="line">            settings[<span class="string">'MONGODB_SERVER'</span>],</div><div class="line">            settings[<span class="string">'MONGODB_PORT'</span>]</div><div class="line">        )</div><div class="line">        db = connection[settings[<span class="string">'MONGODB_DB'</span>]]</div><div class="line">        self.collection = db[settings[<span class="string">'MONGODB_COLLECTION'</span>]]</div></pre></td></tr></table></figure>

<p>接下来定义一个处理函数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></div><div class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> item:</div><div class="line">        <span class="keyword">if</span> <span class="keyword">not</span> data:</div><div class="line">            <span class="keyword">raise</span> DropItem(<span class="string">"Missing data!"</span>)</div><div class="line">    self.collection.update({<span class="string">'url'</span>: item[<span class="string">'url'</span>]}, dict(item), upsert=<span class="keyword">True</span>)</div><div class="line">    log.msg(<span class="string">"Question added to MongoDB database!"</span>,</div><div class="line">            level=log.DEBUG, spider=spider)</div><div class="line">    <span class="keyword">return</span> item</div></pre></td></tr></table></figure>

<p>ok,搞定，我们再测试一把</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">scrapy crawl stack</div></pre></td></tr></table></figure>

<h2 id="执行效果如下">执行效果如下</h2>
<p><img src="http://ww1.sinaimg.cn/large/692869a3gw1erjv5to5w3j20w70g8wio.jpg" alt="mongodb数据库管理"></p>
<h1 id="参考文献">参考文献</h1>
<p>1 <a href="https://github.com/scrapy/scrapy/issues/958" target="_blank" rel="external">ImportError: Error loading object ‘scrapy.contrib.memusage.MemoryUsage’: No module named mail.smtp</a><br>2 <a href="http://stackoverflow.com/questions/8671071/error-to-execute-python-scrappy-module" target="_blank" rel="external">http://stackoverflow.com/questions/8671071/error-to-execute-python-scrappy-module</a><br>3 <a href="http://python.jobbole.com/81320/" target="_blank" rel="external">Python下用Scrapy和MongoDB构建爬虫系统（1）</a></p>

      
    </div>
    <footer>
      
        
  
  <div class="categories">
    <a href="/categories/python学习笔记/">python学习笔记</a>
  </div>

        
  
  <div class="tags">
    <a href="/tags/python/">python</a>, <a href="/tags/scrapy/">scrapy</a>, <a href="/tags/爬虫/">爬虫</a>
  </div>

        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>


 <nav id="pagination" >
    
    <a href="/2015/04/28/ubuntu-denyhosts/" class="alignleft prev" > ubuntu 14.04 安装denyhosts </a>
    
    
    <a href="/2015/04/26/face-detection/" class="alignright next" > 使用opencv实现人脸检测 </a>
    
    <div class="clearfix"></div>
</nav>
<section id="comments">
    <!-- 多说评论框 start -->
    <div class="ds-thread" data-thread-key="post-ubuntu-scrapy-stackoverflow" data-title="ubuntu下使用scrapy抓取cnblogs" data-url="http://jackroyal.github.io/2015/04/26/ubuntu-scrapy-stackoverflow/"></div>
    <!-- 多说评论框 end -->
    <!-- 多说公共JS代码 start (一个网页只需插入一次) -->
    <script type="text/javascript">
    var duoshuoQuery = {short_name:'jackroyal'};
      (function() {
        var ds = document.createElement('script');
        ds.type = 'text/javascript';ds.async = true;
        ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
        ds.charset = 'UTF-8';
        (document.getElementsByTagName('head')[0]
         || document.getElementsByTagName('body')[0]).appendChild(ds);
      })();
      </script>
    <!-- 多说公共JS代码 end -->
  </section>


</div></div>
    <aside id="sidebar" class="alignright">
  <div class="search">
  <form action="//google.com/search" method="get" accept-charset="utf-8">
    <input type="search" name="q" results="0" placeholder="搜索">
    <input type="hidden" name="q" value="site:jackroyal.github.io">
  </form>
</div>

  
<div class="widget tag">
  <h3 class="title">分类</h3>
  <ul class="entry">
  
    <li><a href="/categories/Linux/">Linux</a><small>5</small></li>
  
    <li><a href="/categories/chrome扩展/">chrome扩展</a><small>4</small></li>
  
    <li><a href="/categories/hexo/">hexo</a><small>3</small></li>
  
    <li><a href="/categories/javascript/">javascript</a><small>2</small></li>
  
    <li><a href="/categories/python学习笔记/">python学习笔记</a><small>5</small></li>
  
    <li><a href="/categories/图像处理/">图像处理</a><small>3</small></li>
  
    <li><a href="/categories/数据库/">数据库</a><small>1</small></li>
  
    <li><a href="/categories/服务器/">服务器</a><small>1</small></li>
  
    <li><a href="/categories/杂文/">杂文</a><small>1</small></li>
  
    <li><a href="/categories/软件安装与技巧/">软件安装与技巧</a><small>1</small></li>
  
  </ul>
</div>


  
<div class="widget tag">
  <h3 class="title">标签</h3>
  <ul class="entry">
  
    <li><a href="/tags/Linux/">Linux</a><small>6</small></li>
  
    <li><a href="/tags/chrome/">chrome</a><small>4</small></li>
  
    <li><a href="/tags/cmcc-edu/">cmcc-edu</a><small>4</small></li>
  
    <li><a href="/tags/github/">github</a><small>3</small></li>
  
    <li><a href="/tags/hexo/">hexo</a><small>5</small></li>
  
    <li><a href="/tags/javascript/">javascript</a><small>2</small></li>
  
    <li><a href="/tags/js/">js</a><small>1</small></li>
  
    <li><a href="/tags/kali/">kali</a><small>2</small></li>
  
    <li><a href="/tags/lbph/">lbph</a><small>1</small></li>
  
    <li><a href="/tags/opencv/">opencv</a><small>3</small></li>
  
    <li><a href="/tags/python/">python</a><small>7</small></li>
  
    <li><a href="/tags/scrapy/">scrapy</a><small>1</small></li>
  
    <li><a href="/tags/select/">select</a><small>1</small></li>
  
    <li><a href="/tags/shadowsocks/">shadowsocks</a><small>1</small></li>
  
    <li><a href="/tags/socket/">socket</a><small>2</small></li>
  
    <li><a href="/tags/sql/">sql</a><small>1</small></li>
  
    <li><a href="/tags/sql-server/">sql server</a><small>1</small></li>
  
    <li><a href="/tags/ubuntu/">ubuntu</a><small>2</small></li>
  
    <li><a href="/tags/wamp/">wamp</a><small>1</small></li>
  
    <li><a href="/tags/windows/">windows</a><small>1</small></li>
  
    <li><a href="/tags/中国移动/">中国移动</a><small>3</small></li>
  
    <li><a href="/tags/前端/">前端</a><small>2</small></li>
  
    <li><a href="/tags/图像处理/">图像处理</a><small>3</small></li>
  
    <li><a href="/tags/扩展/">扩展</a><small>2</small></li>
  
    <li><a href="/tags/正则/">正则</a><small>1</small></li>
  
    <li><a href="/tags/正则表达式/">正则表达式</a><small>1</small></li>
  
    <li><a href="/tags/爬虫/">爬虫</a><small>2</small></li>
  
    <li><a href="/tags/脚本/">脚本</a><small>2</small></li>
  
    <li><a href="/tags/表单/">表单</a><small>1</small></li>
  
  </ul>
</div>


  
<div class="widget tag">
  <h3 class="title">最新文章</h3>
  <ul class="entry">
    
      <li>
        <a href="/2015/05/06/opencv-lbph-source-code-analysis/">opencv中LBPH的人脸识别代码分析</a>
      </li>
    
      <li>
        <a href="/2015/04/28/ubuntu-denyhosts/">ubuntu 14.04 安装denyhosts</a>
      </li>
    
      <li>
        <a href="/2015/04/26/ubuntu-scrapy-stackoverflow/">ubuntu下使用scrapy抓取cnblogs</a>
      </li>
    
      <li>
        <a href="/2015/04/26/face-detection/">使用opencv实现人脸检测</a>
      </li>
    
      <li>
        <a href="/2015/04/26/face-recognizer/">使用opencv实现人脸识别</a>
      </li>
    
  </ul>
</div>


  
<div class="widget tagcloud">
  <h3 class="title">标签云</h3>
  <div class="entry">
    <a href="/tags/Linux/" style="font-size: 18.33px;">Linux</a><a href="/tags/chrome/" style="font-size: 15.00px;">chrome</a><a href="/tags/cmcc-edu/" style="font-size: 15.00px;">cmcc-edu</a><a href="/tags/github/" style="font-size: 13.33px;">github</a><a href="/tags/hexo/" style="font-size: 16.67px;">hexo</a><a href="/tags/javascript/" style="font-size: 11.67px;">javascript</a><a href="/tags/js/" style="font-size: 10.00px;">js</a><a href="/tags/kali/" style="font-size: 11.67px;">kali</a><a href="/tags/lbph/" style="font-size: 10.00px;">lbph</a><a href="/tags/opencv/" style="font-size: 13.33px;">opencv</a><a href="/tags/python/" style="font-size: 20.00px;">python</a><a href="/tags/scrapy/" style="font-size: 10.00px;">scrapy</a><a href="/tags/select/" style="font-size: 10.00px;">select</a><a href="/tags/shadowsocks/" style="font-size: 10.00px;">shadowsocks</a><a href="/tags/socket/" style="font-size: 11.67px;">socket</a><a href="/tags/sql/" style="font-size: 10.00px;">sql</a><a href="/tags/sql-server/" style="font-size: 10.00px;">sql server</a><a href="/tags/ubuntu/" style="font-size: 11.67px;">ubuntu</a><a href="/tags/wamp/" style="font-size: 10.00px;">wamp</a><a href="/tags/windows/" style="font-size: 10.00px;">windows</a><a href="/tags/中国移动/" style="font-size: 13.33px;">中国移动</a><a href="/tags/前端/" style="font-size: 11.67px;">前端</a><a href="/tags/图像处理/" style="font-size: 13.33px;">图像处理</a><a href="/tags/扩展/" style="font-size: 11.67px;">扩展</a><a href="/tags/正则/" style="font-size: 10.00px;">正则</a><a href="/tags/正则表达式/" style="font-size: 10.00px;">正则表达式</a><a href="/tags/爬虫/" style="font-size: 11.67px;">爬虫</a><a href="/tags/脚本/" style="font-size: 11.67px;">脚本</a><a href="/tags/表单/" style="font-size: 10.00px;">表单</a>
  </div>
</div>


  <div class="widget tag">
<h3 class="title">友情链接</h3>
<ul class="entry">
<li><a href="http://blog.csdn.net/jackroyal" title="搁浅St的blog">我的CSDN</a></li>
<li><a href="http://www.cnblogs.com/geqianst" title="搁浅St的blog">我的cnblogs</a></li>
<li><a href="http://blog.csdn.net/happyhuirong" title="笨笨的blog">笨笨的blog</a></li>
</ul>
</div>

</aside>
    <div class="clearfix"></div>
  </div>
  <footer id="footer" class="inner"><div class="alignleft">
  
  &copy; 2015 搁浅St
  
</div>
<div class="clearfix"></div></footer>
  <script src="https://lib.sinaapp.com/js/jquery/2.0.3/jquery-2.0.3.min.js"></script>
<script src="/js/jquery.imagesloaded.min.js"></script>
<script src="/js/gallery.js"></script>




<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
(function($){
  $('.fancybox').fancybox();
})(jQuery);
</script>


</body>
</html>